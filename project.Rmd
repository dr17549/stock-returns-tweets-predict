
```{r}
library(quanteda)
library(ggrepel)
library(textclean)
library(tidyverse)
library(glmnet)
library(pROC)
library(knitr)
library(dplyr)
library(dtplyr)
library(data.table)
library(lubridate)
library(xts)
library(PerformanceAnalytics)
library(knitr)
library(kableExtra)
```



```{r}
df <- fread("full_dataset-release.csv")
```

```{r}
#renaming columns

df <- rename(df, TWEET = tweet)

```


```{r}

faang <- my_data %>%
  filter(STOCK %in% c("Facebook", "Apple", "Amazon", "Netflix", "Google"))



```

```{r}
faang <- faang %>%
  mutate(tweet_wordcount=str_count(TWEET,"[[:alpha:]]+"))
```


```{r}
my_data <- my_data %>%
  mutate(tweet_wordcount=str_count(TWEET,"[[:alpha:]]+"))
```

```{r}
length(unique(my_data$STOCK))
```
```{r}

grouped_df <- my_data %>%
  group_by(STOCK, DATE) %>%
  summarize(num_unique_values = n_distinct(1_DAY_RETURN), .groups = 'drop')
  
  
  
```

```{r}
grouped_df <- my_data %>%
```


```{r}
sum(is.na(my_data))
```


```{r}
TMEF_dfm<-function(text,
                   ngrams=1,
                   stop.words=TRUE,
                   min.prop=.01){
  # First, we check our input is correct
  if(!is.character(text)){  
    stop("Must input character vector")
  }
  drop_list=""
  #uses stop.words arugment to adjust what is dropped
  if(stop.words) drop_list=stopwords("en") 
  # quanteda pipeline
  text_data<-text %>%
    replace_contraction() %>%
    tokens(remove_numbers=TRUE,
           remove_punct = TRUE) %>%
    tokens_wordstem() %>%
    tokens_select(pattern = drop_list, 
                  selection = "remove") %>%
    tokens_ngrams(ngrams) %>%
    dfm() %>%
    dfm_trim(min_docfreq = min.prop,docfreq_type="prop")
  return(text_data)
}
```


```{r}
sub_data <- 
```


```{r}



mini <- sample(1:nrow(faang),20000)

faang_small<-faang%>%
  slice(mini)

faang_small$made_money <- ifelse(faang_small$"1_DAY_RETURN" < 0, 0, 1)

train_split <- sample(1:nrow(faang_small),10000)

my_data_sample<-faang_small%>%
  slice(train_split)

test_data<-faang_small %>%
  slice(-train_split)


faang_train_Y<-my_data_sample %>%
  pull(made_money)

test_Y<-test_data %>%
  pull(made_money)

dfm_faang_train_pros <-TMEF_dfm(my_data_sample$TWEET,ngrams=1:2) %>%
  convert(to="matrix")

faang_model_pros<-cv.glmnet(x=dfm_faang_train_pros,
                             y=faang_train_Y)

# check the tuning to see if there is useful information
plot(amazon_model_pros)



```
```{r}


dfm_faang_test_pros<-TMEF_dfm(test_data$TWEET,
                               ngrams=1:2,
                               min.prop = 0) %>%
  dfm_match(colnames(dfm_faang_train_pros)) %>%
  convert(to="matrix")

faang_test_predict_pros<-predict(faang_model_pros,
                                  newx = dfm_faang_test_pros)[,1]
hist(amazon_test_predict_pros)
hist(test_Y)


pros_acc<-kendall_acc(faang_test_predict_pros,test_Y)

pros_acc
```

```{r}
kendall_acc<-function(x,y,percentage=TRUE){
  kt=cor(x,y,method="kendall")
  kt.acc=.5+kt/2
  kt.se=sqrt((kt.acc*(1-kt.acc))/length(x))
  report=data.frame(acc=kt.acc,
                    lower=kt.acc-1.96*kt.se,
                    upper=kt.acc+1.96*kt.se)
  report = round(report,4)
  if(percentage) report = report*100
  return(report)
}
```


