
```{r}
library(quanteda)
library(ggrepel)
library(textclean)
library(tidyverse)
library(glmnet)
library(pROC)
library(knitr)
library(dplyr)
library(dtplyr)
library(data.table)
library(lubridate)
library(xts)
library(PerformanceAnalytics)
library(knitr)
library(kableExtra)
library(dplyr)
library(stm)
source("TMEF_dfm.R")

# Classifiers
library(naivebayes)
```



```{r}
df <- fread("full_dataset-release.csv")
```

```{r}
#renaming columns
df <- df %>%
  rename(
    tweet = TWEET,
    stock = STOCK,
    one_day_return ="1_DAY_RETURN", 
    seven_day_return = "7_DAY_RETURN",
    date = DATE
  )

df <- df %>%
  mutate(date = as.Date(date, format = "%d/%m/%Y"))

```





```{r}
rows_with_null <- sum(!complete.cases(df))
print(rows_with_null)

#since they are only 150, we will drop all rows with missing data

df <- na.omit(df)

#Some other data pre-processing: 
#Remove html links: 
df$tweet <- replace_url(df$tweet)

```
The function replace_emoji replaces emojis with text representations 
while replace_emoji_identifier replaces with a unique identifier that corresponds to
lexicon::hash_sentiment_emoji for use in the sentimentr package.


```{r}
df_emoji_text <- df
df_emoji_identifier <- df
df_emoji_text$tweet <- replace_emoji(df$tweet)
df_emoji_identifier$tweet<- replace_emoji(df$tweet) 
```


```{r}
replaced_text <- replace_emoji("hello ðŸ˜ ")
```



Let's Do some sanity checks on the data and check for consistency. 

Every tweet of the same company on the same day should have the same price/ volatility/ basically all other meta data



# Number of Tweets per company 
```{r}
tweet_counts <- df %>%
  group_by(stock) %>%
  summarise(num_tweets = n())

tweet_counts <- tweet_counts %>%
  arrange(desc(num_tweets))

top_10 <- head(tweet_counts, 10)
lowest_10 <- tail(tweet_counts, 10)

ggplot(top_10, aes(x = reorder(stock, num_tweets), y = num_tweets)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  geom_text(aes(label = num_tweets), vjust = -0.5, size = 3) +
  labs(x = "Company", y = "Number of Tweets", title = "Top 10 Companies by Number of Tweets") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggplot(lowest_10, aes(x = reorder(stock, num_tweets), y = num_tweets)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  geom_text(aes(label = num_tweets), vjust = -0.5, size = 3) +
  labs(x = "Company", y = "Number of Tweets", title = "Lowest 10 Companies by Number of Tweets") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


faang_tweet_counts <- df %>%
  filter(stock %in% c("Facebook", "Apple", "Amazon", "Netflix", "Google")) %>%
  group_by(stock) %>%
  summarise(num_tweets = n())


ggplot(faang_tweet_counts, aes(x = reorder(stock, num_tweets), y = num_tweets)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  geom_text(aes(label = num_tweets), vjust = -0.5, size = 3) +
  labs(x = "Company", y = "Number of Tweets", title = "FAANG by Number of Tweets") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```



# Number of Tweets with Positive vs. Negative Sentiment (According to LTSM and Textblob Polarity)

```{r}
# Calculate summary statistics for LSTM_POLARITY and TEXTBLOB_POLARITY by stock
sentiment_summary <- df %>%
  group_by(stock) %>%
  summarise(
    avg_lstm_polarity = mean(LSTM_POLARITY, na.rm = TRUE),
    avg_textblob_polarity = mean(TEXTBLOB_POLARITY, na.rm = TRUE),
    total_tweets = n()
  )

# Visualize sentiment scores for each stock
# You can use bar plots or other visualizations as per your preference
ggplot(sentiment_summary, aes(x = reorder(stock, avg_lstm_polarity), y = avg_lstm_polarity)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(x = "Stock", y = "Average LSTM Polarity", title = "Average LSTM Polarity by Stock") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggplot(sentiment_summary, aes(x = reorder(stock, avg_textblob_polarity), y = avg_textblob_polarity)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(x = "Stock", y = "Average TextBlob Polarity", title = "Average TextBlob Polarity by Stock") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


faang_sentiment_summary <- sentiment_summary %>%
    filter(stock %in% c("Facebook", "Apple", "Amazon", "Netflix", "Google")) 
 

ggplot(faang_sentiment_summary, aes(x = reorder(stock, avg_lstm_polarity), y = avg_lstm_polarity)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(x = "Stock", y = "FAANG Average LSTM Polarity", title = "FAANG Average LSTM Polarity by Stock") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggplot(faang_sentiment_summary, aes(x = reorder(stock, avg_textblob_polarity), y = avg_textblob_polarity)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(x = "Stock", y = "FAANG Average TextBlob Polarity", title = "FAANG Average TextBlob Polarity by Stock") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

```


# Focusing on FAANG 

Since all the FAANGs have positive sentiment, training on FAANG might not yield great results for transfer learning because our model may not be exposed to negative sentiment 

Let's Zoom into FAANG: 
```{r}

faang <- df %>%
  filter(stock %in% c("Facebook", "Apple", "Amazon", "Netflix", "Google"))

```

Across Dates: 
```{r}
tweet_counts <- faang %>%
  filter(year(date) == 2017) %>%
  group_by(date) %>%
  summarise(num_tweets = n())

# Plot the number of tweets per date
ggplot(tweet_counts, aes(x = date, y = num_tweets)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(x = "Date", y = "Number of Tweets", title = "Number of Tweets per Date in 2017") +
  theme_minimal() 
```
Interesting we see that the vast majority of the tweets come  either the first or last day of the month for 2017

```{r}
tweet_counts <- faang %>%
  filter(year(date) == 2018) %>%
  group_by(date) %>%
  summarise(num_tweets = n())

# Plot the number of tweets per date
ggplot(tweet_counts, aes(x = date, y = num_tweets)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(x = "Date", y = "Number of Tweets", title = "Number of Tweets per Date in 2018") +
  theme_minimal() 
```
For 2018 it is much more evenly distributed across the months, however we also see some gaps in the data, such as the first half of the year and the month of October. 

Let's look at Tweet Word Count: 
```{r}
df <- df %>%
  mutate(tweet_wordcount=str_count(tweet,"[[:alpha:]]+"))


summarized <-   df %>%
                group_by(stock) %>%
                summarise(avg = mean(tweet_wordcount, na.rm = TRUE))

# Print the result
print(summarized)
print(mean(summarized$avg))
print(sd(summarized$avg))
```


# Build a DFM for FAANG 
```{r}
DFM <- TMEF_dfm(faang$tweet,ngrams=1:2)
```

```{r}
length(unique(my_data$STOCK))
```

```{r}
sum(is.na(my_data))
```

```{r}
word_freq <- colSums(DFM)

# Convert the frequencies to a dataframe
word_freq_df <- data.frame(word = names(word_freq), freq = word_freq)

# Sort the dataframe by frequency in descending order
word_freq_df <- word_freq_df[order(-word_freq_df$freq), ]

# Plot the top N most common words
top_n <- 50  # Adjust this value to plot more or fewer words
ggplot(head(word_freq_df, top_n), aes(x = reorder(word, freq), y = freq)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(x = "Word", y = "Frequency", title = "Top 20 Most Common Words") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
custom_stop <- c('amazon','Facebook', '@amazon', 'netflix', '@netflix', 'apple', '@apple', 'facebook', '@facebook', 'google', '@google', 'rt','RT','Rt')

custom_dfm 


```


Some further Data Exploration using 
```{r}
custom_stop <- c('amazon','Facebook', '@amazon', 'netflix', '@netflix', 'apple', '@apple', 'facebook', '@facebook', 'google', '@google', 'rt','RT','Rt')

facebook <- df %>%
  filter(stock %in% c("Facebook")) %>%
  filter(!is.na(tweet) & tweet != "")


train_split=sample(1:nrow(facebook),0.8*nrow(facebook))

facebook_train<-facebook[train_split,]
facebook_test<-facebook[-train_split,]

facebook_dfm_train<-TMEF_dfm(facebook_train$tweet,ngrams=1,custom_stop_words = custom_stop)

```

```{r}

facebook_topicMod7<-stm(faang_small_dfm_train,K=7)
topicNum=facebook_topicMod7$settings$dim$K
topicNames<-paste0("Topic",1:topicNum)

# Most common topics, and most common words from each topic
labelTopics(facebook_topicMod7)

```












```{r}
custom_stop <- c('amazon','Facebook', '@amazon', 'netflix', '@netflix', 'apple', '@apple', 'facebook', '@facebook', 'google', '@google', 'rt','RT','Rt')
amazon <- df %>%
  filter(stock %in% c("Amazon")) %>%
  filter(!is.na(tweet) & tweet != "")


train_split=sample(1:nrow(amazon),0.8*nrow(amazon))

amazon_train<-amazon[train_split,]
amazon_test<-amazon[-train_split,]

amazon_dfm_train<-TMEF_dfm(amazon_train$tweet,ngrams=1,custom_stop_words = custom_stop)


amazon_topicMod7<-stm(amazon_dfm_train,K=7)
topicNum=amazon_topicMod7$settings$dim$K
topicNames<-paste0("Topic",1:topicNum)

# Most common topics, and most common words from each topic
labelTopics(amazon_topicMod7)

```



# N-grams daily No company name 
```{r}

custom_stop <- c('amazon','Facebook', '@amazon', 'netflix', '@netflix', 'apple', '@apple', 'facebook', '@facebook', 'google', '@google', 'rt','RT','Rt')

faang$made_money <- ifelse(faang$"one_day_return" < 0, 0, 1)

train_split <- sample(1:nrow(faang),0.8*nrow(faang))

faang_train_data<-faang%>%
  slice(train_split)

faang_test_data<-faang %>%
  slice(-train_split)

faang_train_Y<-faang_train_data %>%
  pull(made_money)

test_Y<-faang_test_data %>%
  pull(made_money)

dfm_faang_train_no_company_name<-TMEF_dfm(faang_train_data$tweet,ngrams=1:2, custom_stop_words = custom_stop) %>%
  convert(to="matrix")

faang_model_pros<-cv.glmnet(x=dfm_faang_train_no_company_name,
                             y=faang_train_Y)

```

```{r}
dfm_faang_test_no_company_name<-TMEF_dfm(faang_test_data$tweet,
                               ngrams=1:2,
                               min.prop = 0, custom_stop_words = custom_stop) %>%
  dfm_match(colnames(dfm_faang_train_no_company_name)) %>%
  convert(to="matrix")

faang_test_predict_pros<-predict(faang_model_pros,
                                  newx = dfm_faang_test_no_company_name)[,1]

hist(test_Y)

head(faang_test_predict_pros)


test_predict_binary <-  ifelse( faang_test_predict_pros< 0.5, 0, 1)
```


```{r}
round(100*mean(test_predict_binary==test_Y),3)

conf_matrix <- table(Actual = test_Y, Predicted = test_predict_binary)

conf_matrix


```
```{r}
# lots of zeros
faang_model_pros %>%
  coef() %>%
  drop()

# let's get this in a data frame
faang_model_pros %>%
  coef() %>%
  drop() %>%
  as.data.frame() %>%
  rownames_to_column(var = "ngram") %>%
  rename(score=".")

# just the top
faang_model_pros %>%
  coef() %>%
  drop() %>%
  as.data.frame() %>%
  rownames_to_column(var = "ngram") %>%
  rename(score=".") %>%
  head(20)

# drop zeros, and save
plotCoefs<-faang_model_pros %>%
  coef() %>%
  drop() %>%
  as.data.frame() %>%
  rownames_to_column(var = "ngram") %>%
  rename(score=".") %>%
  filter(score!=0 & ngram!="(Intercept)" & !is.na(score))  

plotCoefs

# create a similar data frame with ngram frequencies
plotFreqs<-data.frame(ngram=colnames(dfm_faang_train),
                      freq=colMeans(dfm_faang_train))


# combine data, round for easy reading
plotDat<-plotCoefs %>%
  left_join(plotFreqs) %>%
  mutate_at(vars(score,freq),~round(.,3))

head(plotDat)



plotDat %>%
  ggplot(aes(x=score,y=freq,label=ngram,color=score)) +
  scale_color_gradient2(low="navyblue",
                        mid = "grey",
                        high="forestgreen",
                        midpoint = 0)+
  geom_vline(xintercept=0)+
  geom_point() +
  geom_label_repel(max.overlaps = 15)+  
  scale_x_continuous(limits = c(-.2,.1),
                     breaks = seq(-.2,.2,.05)) +
  scale_y_continuous(trans="log2",
                     breaks=c(.01,.05,.1,.2,.5,1,2,5))+
  theme_bw() +
  labs(x="Coefficient in Model",y="Uses per Review")+
  theme(legend.position = "none",
        axis.title=element_text(size=20),
        axis.text=element_text(size=16))

ggsave("week2.png",dpi=200,width=20,height=10)
```


#Non-binary
```{r}
faang_train_Y<-faang_train_data %>%
  pull(one_day_return)

test_Y<-faang_test_data %>%
  pull(one_day_return)



faang_model_pros<-cv.glmnet(x=dfm_faang_train,
                             y=faang_train_Y)


faang_test_predict_pros<-predict(faang_model_pros,
                                  newx = dfm_faang_test_pros)[,1]

hist(test_Y)

pros_acc<-kendall_acc(faang_test_predict_pros,test_Y)



```

```{r}
kendalls_acc<-kendall_acc(faang_test_predict_pros,test_Y)

kendalls_acc
```


# N-grams weekly
```{r}
weekly_aggregated_faang <- faang %>%
  group_by(week_start_date = lubridate::floor_date(date, unit = "week"), stock) %>%
  summarize(
    aggregated_tweets = paste(tweet, collapse = " "),
    seven_day_return = first(seven_day_return),
    num_tweets = n(),
    .groups = "drop_last"
  ) %>%
  ungroup()
```


```{r}
weekly_aggregated_faang$made_money <- ifelse(weekly_aggregated_faang$seven_day_return < 0, 0, 1)

train_split <- sample(1:nrow(weekly_aggregated_faang),0.8*nrow(weekly_aggregated_faang))

faang_train_data_weekly<-weekly_aggregated_faang %>%
  slice(train_split)

faang_test_data_weekly<-weekly_aggregated_faang %>%
  slice(-train_split)

faang_train_Y_weekly<-faang_train_data_weekly %>%
  pull(made_money)

test_Y_weekly<-faang_test_data_weekly %>%
  pull(made_money)

dfm_faang_train_weekly<-TMEF_dfm(faang_train_data_weekly$aggregated_tweets,ngrams=1:2) %>%
  convert(to="matrix")



```
```{r}

```


```{r}
faang_model_weekly <-cv.glmnet(x=dfm_faang_train_weekly,
                             y=faang_train_Y_weekly)

dfm_faang_test_weekly<-TMEF_dfm(faang_test_data_weekly$aggregated_tweets,
                               ngrams=1:2,
                               min.prop = 0) %>%
  dfm_match(colnames(dfm_faang_train_weekly)) %>%
  convert(to="matrix")



```



```{r}
faang_test_predict_weekly<-predict(faang_model_weekly,
                                  newx = dfm_faang_test_weekly)[,1]


test_predict_binary <-  ifelse( faang_test_predict_weekly < 0.5, 0, 1)

round(100*mean(test_predict_binary==test_Y_weekly),3)

conf_matrix <- table(Actual = test_Y_weekly, Predicted = test_predict_binary)
```
```{r}
test_predict_binary
```

```{r}
weekly_aggregated_whole <- df_emoji_text %>%
  group_by(week_start_date = lubridate::floor_date(date, unit = "week"), stock) %>%
  summarize(
    aggregated_tweets = paste(tweet, collapse = " "),
    seven_day_return = first(seven_day_return),
    num_tweets = n(),
    .groups = "drop_last"
  ) %>%
  ungroup()


weekly_aggregated_whole$made_money <- ifelse(weekly_aggregated_whole$seven_day_return < 0, 0, 1)

train_split <- sample(1:nrow(weekly_aggregated_whole),0.8*nrow(weekly_aggregated_whole))

train_data_weekly<-weekly_aggregated_whole %>%
  slice(train_split)

test_data_weekly<-weekly_aggregated_whole %>%
  slice(-train_split)

train_Y_whole<-train_data_weekly %>%
  pull(made_money)

test_Y_weekly<-test_data_weekly %>%
  pull(made_money)

dfm_train_weekly<-TMEF_dfm(train_data_weekly$aggregated_tweets,ngrams=1:2) %>%
  convert(to="matrix")
```




# Naive Bayes 
```{r}
# Change format of the dataset so it works with the naive bayes function 
naive_bayes_train <- as.data.frame(cbind(dfm_faang_train_pros, faang_train_Y))
naive_bayes_train$faang_train_Y <- as.factor(df_from_array$faang_train_Y)
faang_model_pros <- naive_bayes(faang_train_Y ~ ., data = naive_bayes_train)

# Change format of the test dataset so it works with the naive bayes function 
naive_bayes_test <- as.data.frame(cbind(dfm_faang_test_pros, test_Y))
naive_bayes_test$faang_train_Y <- as.factor(df_from_array$faang_train_Y)

faang_test_predict_pros<-predict(faang_model_pros,newdata = df_test)
head(faang_test_predict_pros)

# Accuracy 
round(100*mean(faang_test_predict_pros==test_Y),3)

```


