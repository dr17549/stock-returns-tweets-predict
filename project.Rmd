
```{r}
library(quanteda)
library(ggrepel)
library(textclean)
library(tidyverse)
library(glmnet)
library(pROC)
library(knitr)
library(dplyr)
library(dtplyr)
library(data.table)
library(lubridate)
library(xts)
library(PerformanceAnalytics)
library(knitr)
library(kableExtra)
library(dplyr)
library(stm)
library(ROSE)
library(spacyr)
source("TMEF_dfm.R")



# Classifiers
library(naivebayes)
```



```{r}
df <- fread("full_dataset-release.csv")
```




```{r}
#renaming columns
df <- df %>%
  rename(
    tweet = TWEET,
    stock = STOCK,
    one_day_return ="1_DAY_RETURN", 
    seven_day_return = "7_DAY_RETURN",
    date = DATE
  )

df <- df %>%
  mutate(date = as.Date(date, format = "%d/%m/%Y"))

```

```{r}
industries <- read_csv("industries.csv")

```
```{r}
merged_df <- merge(df, industries, by = "stock", all.x = TRUE)

# View the merged data frame
head(merged_df)
```





```{r}
rows_with_null <- sum(!complete.cases(merged_df))
print(rows_with_null)

#since they are only 150, we will drop all rows with missing data

df <- na.omit(merged_df)

#Some other data pre-processing: 
#Remove html links: 
merged_df$tweet <- replace_url(merged_df$tweet)

```
The function replace_emoji replaces emojis with text representations 
while replace_emoji_identifier replaces with a unique identifier that corresponds to
lexicon::hash_sentiment_emoji for use in the sentimentr package.


```{r}
df_emoji_text <- df
df_emoji_identifier <- df
df_emoji_text$tweet <- replace_emoji(df$tweet)
df_emoji_identifier$tweet<- replace_emoji(df$tweet) 
```





Let's Do some sanity checks on the data and check for consistency. 

Every tweet of the same company on the same day should have the same price/ volatility/ basically all other meta data



# Number of Tweets per company 
```{r}
tweet_counts <- df %>%
  group_by(stock) %>%
  summarise(num_tweets = n())

tweet_counts <- tweet_counts %>%
  arrange(desc(num_tweets))

top_10 <- head(tweet_counts, 10)
lowest_10 <- tail(tweet_counts, 10)

ggplot(top_10, aes(x = reorder(stock, num_tweets), y = num_tweets)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  geom_text(aes(label = num_tweets), vjust = -0.5, size = 3) +
  labs(x = "Company", y = "Number of Tweets", title = "Top 10 Companies by Number of Tweets") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggplot(lowest_10, aes(x = reorder(stock, num_tweets), y = num_tweets)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  geom_text(aes(label = num_tweets), vjust = -0.5, size = 3) +
  labs(x = "Company", y = "Number of Tweets", title = "Lowest 10 Companies by Number of Tweets") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


faang_tweet_counts <- df %>%
  filter(stock %in% c("Facebook", "Apple", "Amazon", "Netflix", "Google")) %>%
  group_by(stock) %>%
  summarise(num_tweets = n())


ggplot(faang_tweet_counts, aes(x = reorder(stock, num_tweets), y = num_tweets)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  geom_text(aes(label = num_tweets), vjust = -0.5, size = 3) +
  labs(x = "Company", y = "Number of Tweets", title = "FAANG by Number of Tweets") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```



# Number of Tweets with Positive vs. Negative Sentiment (According to LTSM and Textblob Polarity)

```{r}
# Calculate summary statistics for LSTM_POLARITY and TEXTBLOB_POLARITY by stock
sentiment_summary <- df %>%
  group_by(stock) %>%
  summarise(
    avg_lstm_polarity = mean(LSTM_POLARITY, na.rm = TRUE),
    avg_textblob_polarity = mean(TEXTBLOB_POLARITY, na.rm = TRUE),
    total_tweets = n()
  )

# Visualize sentiment scores for each stock
# You can use bar plots or other visualizations as per your preference
ggplot(sentiment_summary, aes(x = reorder(stock, avg_lstm_polarity), y = avg_lstm_polarity)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(x = "Stock", y = "Average LSTM Polarity", title = "Average LSTM Polarity by Stock") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggplot(sentiment_summary, aes(x = reorder(stock, avg_textblob_polarity), y = avg_textblob_polarity)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(x = "Stock", y = "Average TextBlob Polarity", title = "Average TextBlob Polarity by Stock") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


faang_sentiment_summary <- sentiment_summary %>%
    filter(stock %in% c("Facebook", "Apple", "Amazon", "Netflix", "Google")) 
 

ggplot(faang_sentiment_summary, aes(x = reorder(stock, avg_lstm_polarity), y = avg_lstm_polarity)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(x = "Stock", y = "FAANG Average LSTM Polarity", title = "FAANG Average LSTM Polarity by Stock") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

ggplot(faang_sentiment_summary, aes(x = reorder(stock, avg_textblob_polarity), y = avg_textblob_polarity)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(x = "Stock", y = "FAANG Average TextBlob Polarity", title = "FAANG Average TextBlob Polarity by Stock") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

```


# Focusing on FAANG 

Since all the FAANGs have positive sentiment, training on FAANG might not yield great results for transfer learning because our model may not be exposed to negative sentiment 

Let's Zoom into FAANG: 
```{r}

faang <- merged_df %>%
  filter(stock %in% c("Facebook", "Apple", "Amazon", "Netflix", "Google"))

```

Across Dates: 
```{r}
tweet_counts <- faang %>%
  filter(year(date) == 2017) %>%
  group_by(date) %>%
  summarise(num_tweets = n())

# Plot the number of tweets per date
ggplot(tweet_counts, aes(x = date, y = num_tweets)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(x = "Date", y = "Number of Tweets", title = "Number of Tweets per Date in 2017") +
  theme_minimal() 
```
Interesting we see that the vast majority of the tweets come  either the first or last day of the month for 2017

```{r}
tweet_counts <- faang %>%
  filter(year(date) == 2018) %>%
  group_by(date) %>%
  summarise(num_tweets = n())

# Plot the number of tweets per date
ggplot(tweet_counts, aes(x = date, y = num_tweets)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(x = "Date", y = "Number of Tweets", title = "Number of Tweets per Date in 2018") +
  theme_minimal() 
```
For 2018 it is much more evenly distributed across the months, however we also see some gaps in the data, such as the first half of the year and the month of October. 

Let's look at Tweet Word Count: 
```{r}
df <- df %>%
  mutate(tweet_wordcount=str_count(tweet,"[[:alpha:]]+"))


summarized <-   df %>%
                group_by(stock) %>%
                summarise(avg = mean(tweet_wordcount, na.rm = TRUE))

# Print the result
print(summarized)
print(mean(summarized$avg))
print(sd(summarized$avg))
```


# Build a DFM for FAANG 
```{r}
DFM <- TMEF_dfm(faang$tweet,ngrams=1:2)
```

```{r}
length(unique(my_data$STOCK))
```

```{r}
sum(is.na(my_data))
```

```{r}
word_freq <- colSums(DFM)

# Convert the frequencies to a dataframe
word_freq_df <- data.frame(word = names(word_freq), freq = word_freq)

# Sort the dataframe by frequency in descending order
word_freq_df <- word_freq_df[order(-word_freq_df$freq), ]

# Plot the top N most common words
top_n <- 50  # Adjust this value to plot more or fewer words
ggplot(head(word_freq_df, top_n), aes(x = reorder(word, freq), y = freq)) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(x = "Word", y = "Frequency", title = "Top 20 Most Common Words") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
custom_stop <- c('amazon','Facebook', '@amazon', 'netflix', '@netflix', 'apple', '@apple', 'facebook', '@facebook', 'google', '@google', 'rt','RT','Rt')

custom_dfm 


```


Some further Data Exploration using 
```{r}
custom_stop <- c('amazon','Facebook', '@amazon', 'netflix', '@netflix', 'apple', '@apple', 'facebook', '@facebook', 'google', '@google', 'rt','RT','Rt')

facebook <- df %>%
  filter(stock %in% c("Facebook")) %>%
  filter(!is.na(tweet) & tweet != "")


train_split=sample(1:nrow(facebook),0.8*nrow(facebook))

facebook_train<-facebook[train_split,]
facebook_test<-facebook[-train_split,]

facebook_dfm_train<-TMEF_dfm(facebook_train$tweet,ngrams=1,custom_stop_words = custom_stop)

```

```{r}
suppressMessages({
  suppressWarnings({
    facebook_topicMod7 <- stm(faang_small_dfm_train, K = 7)
  })
})
topicNum=facebook_topicMod7$settings$dim$K
topicNames<-paste0("Topic",1:topicNum)

# Most common topics, and most common words from each topic
labelTopics(facebook_topicMod7)

```












```{r}
custom_stop <- c('amazon','Facebook', '@amazon', 'netflix', '@netflix', 'apple', '@apple', 'facebook', '@facebook', 'google', '@google', 'rt','RT','Rt')
amazon <- df %>%
  filter(stock %in% c("Amazon")) %>%
  filter(!is.na(tweet) & tweet != "")


train_split=sample(1:nrow(amazon),0.8*nrow(amazon))

amazon_train<-amazon[train_split,]
amazon_test<-amazon[-train_split,]

amazon_dfm_train<-TMEF_dfm(amazon_train$tweet,ngrams=1,custom_stop_words = custom_stop)


amazon_topicMod7<-stm(amazon_dfm_train,K=7)
topicNum=amazon_topicMod7$settings$dim$K
topicNames<-paste0("Topic",1:topicNum)

# Most common topics, and most common words from each topic
labelTopics(amazon_topicMod7)

```



# N-grams daily No company name 
```{r}

custom_stop <- c('amazon','Facebook', '@amazon', 'netflix', '@netflix', 'apple', '@apple', 'facebook', '@facebook', 'google', '@google', 'rt','RT','Rt')

table(faang$made_money)

# Calculate the number of samples in the minority class
n_minority <- sum(faang$made_money == "0")

# Create a formula with 'made_money' as the response variable
formula <- as.formula("made_money ~ V1 + tweet + stock + date + LAST_PRICE + one_day_return")

# Undersample the majority class to have the same number of samples as the minority class
undersampled_faang <- ovun.sample(formula, data = faang, method = "under")$data



# Check the class distribution after undersampling
table(undersampled_faang$made_money)
```

#Undersampled N-grams daily
```{r}
train_split <- sample(1:nrow(undersampled_faang),0.8*nrow(undersampled_faang))

faang_train_data<-undersampled_faang%>%
  slice(train_split)

faang_test_data<-undersampled_faang %>%
  slice(-train_split)

faang_train_Y<-faang_train_data %>%
  pull(made_money)

test_Y<-faang_test_data %>%
  pull(made_money)

dfm_faang_train_no_company_name<-TMEF_dfm(faang_train_data$tweet,ngrams=1:2, custom_stop_words = custom_stop) %>%
  convert(to="matrix")

faang_model_pros<-cv.glmnet(x=dfm_faang_train_no_company_name,
                             y=faang_train_Y)


dfm_faang_test_no_company_name<-TMEF_dfm(faang_test_data$tweet,
                               ngrams=1:2,
                               min.prop = 0, custom_stop_words = custom_stop) %>%
  dfm_match(colnames(dfm_faang_train_no_company_name)) %>%
  convert(to="matrix")

faang_test_predict_pros<-predict(faang_model_pros,
                                  newx = dfm_faang_test_no_company_name)[,1]

hist(test_Y)

head(faang_test_predict_pros)


test_predict_binary <-  ifelse( faang_test_predict_pros< 0.5, 0, 1)

round(100*mean(test_predict_binary==test_Y),3)

conf_matrix <- table(Actual = test_Y, Predicted = test_predict_binary)

conf_matrix


TP <- sum(test_predict_binary == 1 & test_Y == 1)
FP <- sum(test_predict_binary == 1 & test_Y == 0)
FN <- sum(test_predict_binary == 0 & test_Y == 1)

# Calculate Precision and Recall
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)

precision
recall

```

We only kept the most popular industries and removed Facebook, because it has the most reviews and will skew the data
```{r}
industry_df <- merged_df %>% filter(stock != "Facebook") %>% 
  filter(industry %in% c("Automotive", "E-commerce", "Media", "Retail", "Technology"))
```


```{r}
ggplot(industry_df, aes(x = industry)) +
  geom_bar(fill = "skyblue", color = "black") +
  labs(title = "Distribution of Industry",
       x = "Industry",
       y = "Frequency") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

```{r}
companies <- c("Yahoo", "YHOO",
               "Wells Fargo", "WFC",
               "Walmart", "WMT",
               "Volkswagen", "VOW3.DE",
               "Vodafone", "VOD",
               "Visa", "V",
               "Viacom", "VIAC",
               "Verizon", "VZ",
               "UPS", "UPS",
               "TripAdvisor", "TRIP",
               "Toyota", "TM",
               "TMobile", "TMUS",
               "Thales", "HO.PA",
               "Tesco", "TSCO.L",
               "Starbucks", "SBUX",
               "Sony", "SONY",
               "Siemens", "SIE.DE",
               "Shell", "RDS.A",
               "SAP", "SAP",
               "Santander", "SAN",
               "Samsung", "005930.KS",
               "salesforce.com", "CRM",
               "Ryanair", "RYAAY",
               "Reuters", "TRI",
               "Pfizer", "PFE",
               "Pepsi", "PEP",
               "PayPal", "PYPL",
               "P&G", "PG",
               "Oracle", "ORCL",
               "Nissan", "7201.T",
               "Nike", "NKE",
               "Next", "NXT.L",
               "Netflix", "NFLX",
               "Nestle", "NSRGY",
               "Morgan Stanley", "MS",
               "Microsoft", "MSFT",
               "McDonald's", "MCD",
               "Mastercard", "MA",
               "L'Oreal", "OR",
               "Kroger", "KR",
               "Kellogg's", "K",
               "JPMorgan", "JPM",
               "John Deere", "DE",
               "Intel", "INTC",
               "IBM", "IBM",
               "Hyundai", "005380.KS",
               "HSBC", "HSBC",
               "HP", "HPQ",
               "Honda", "HMC",
               "Home Depot", "HD",
               "Heineken", "HEINY",
               "H&M", "HM-B.ST",
               "GSK", "GSK.L",
               "Groupon", "GRPN",
               "Google", "GOOGL",
               "Goldman Sachs", "GS",
               "Gillette", "GILC34.SA",
               "General Electric", "GE",
               "Ford", "F",
               "FedEx", "FDX",
               "Facebook", "FB",
               "Exxon", "XOM",
               "Expedia", "EXPE",
               "Equinor", "EQNR",
               "eBay", "EBAY",
               "easyJet", "EZJ.L",
               "Disney", "DIS",
               "Deutsche Bank", "DB",
               "Danone", "BN.PA",
               "CVS Health", "CVS",
               "Costco", "COST",
               "Comcast", "CMCSA",
               "Colgate", "CL",
               "CocaCola", "KO",
               "Citigroup", "C",
               "Cisco", "CSCO",
               "Chevron", "CVX",
               "CBS", "VIAC",
               "Carrefour", "CA.PA",
               "Cardinal Health", "CAH",
               "Burberry", "BRBY.L",
               "BP", "BP",
               "bookingcom", "BKNG",
               "Boeing", "BA",
               "BMW", "BMW.DE",
               "BlackRock", "BLK",
               "Bayer", "BAYRY",
               "BASF", "BAS.DE",
               "Bank of America", "BAC",
               "Aviva", "AV.L",
               "Audi", "NSU.DE",
               "AT&T", "T",
               "AstraZeneca", "AZN",
               "ASOS", "ASC.L",
               "Apple", "AAPL",
               "American Express", "AXP",
               "Amazon", "AMZN",
               "Allianz", "ALV.DE",
               "Adobe", "ADBE",
               "adidas", "ADDYY",
               "21CF", "TFCF")

```

#Regular N-grams daily INDUSTRY
```{r}
set.seed(42)

custom_stop <- c('amazon','Facebook', '@amazon', 'netflix', '@netflix', 'apple', '@apple', 'facebook', '@facebook', 'google', '@google', 'rt','RT','Rt')

sample <- industry_df[sample(nrow(industry_df), nrow(industry_df)*0.001), ]
# Splitting the data into train and test sets using categorical splitting
train_split <- sample(1:nrow(sample), 0.8 * nrow(sample))

ind_train_data<-sample%>%
  slice(train_split)

ind_test_data<-sample %>%
  slice(-train_split)

# Extracting target variables


industry_dfm_train_no_comp_names<-TMEF_dfm(ind_train_data$tweet,ngrams=1:2, custom_stop_words = companies, min.prop = 0.0)
industry_dfm_train <-TMEF_dfm(ind_train_data$tweet,ngrams=1:2, min.prop = 0.0)


industry_dfm_test_no_comp_names <-TMEF_dfm(ind_test_data$tweet,
                                  ngrams=1:2, custom_stop_words = companies, min.prop=0) %>%
                                  dfm_match(colnames(industry_dfm_train_no_comp_names))

industry_dfm_test <-TMEF_dfm(ind_test_data$tweet,
                    ngrams=1:2, min.prop=0) %>%
                    dfm_match(colnames(industry_dfm_train))
```

```{r}
train_Y <- ind_train_data$industry
industry_model_no_comp_names <- cv.glmnet(x=industry_dfm_train_no_comp_names,
                                y=train_Y,
                                family="multinomial", alpha = 1)


industry_model <- cv.glmnet(x=industry_dfm_train,
                            y=train_Y,
                            family="multinomial", alpha = 1)
```

```{r}

industry_predict_label_no_comp_names<-predict(industry_model_no_comp_names,
                                     newx = industry_dfm_test_no_comp_names,
                                     type="class")[,1]

industry_predict_label<-predict(industry_model,
                                newx = industry_dfm_test,
                                type="class")[,1]

# raw accuracy
mean(industry_predict_label_no_comp_names == ind_test_data$industry)
mean(industry_predict_label == ind_test_data$industry)
# Confusion matrix - great for multinomials!
table(industry_predict_label,ind_test_data$industry)
```
```{r}
indices_auto <- which(industry_predict_label == "Automotive" & ind_test_data$industry != "Automotive")

# Extract samples where the predicted label is "automotive" but actual label differs
mislabeled_samples_not_auto <- ind_test_data[indices_auto, ]


# View the mislabeled samples
examples <-mislabeled_samples_not_auto %>% filter (V1 %in% c(798214,727155, 165166))
```


```{r}
spacyr::spacy_initialize()
```

```{r}

# Function to extract the subject from a text
extract_subject <- function(text) {
  # Parse the text using SpaCy
  parsed <- spacy_parse(text,
                     lemma = T,
                     dependency = T)
  
  # Extract the main noun (subject) from the parsed data
  main_nouns <- parsed$lemma[parsed$dep == "nsubj"]
  
  if (length(main_nouns) > 1) {
    return(paste(paste0(main_nouns, "_subj"), collapse = ", "))
  }
  
  # Return the main noun (subject)
  if(length(main_nouns) == 0) {
    return("")
  }
  return(paste0(main_nouns, "_subj"))
}

```




```{r}
set.seed(42)

custom_stop <- c('amazon','Facebook', '@amazon', 'netflix', '@netflix', 'apple', '@apple', 'facebook', '@facebook', 'google', '@google', 'rt','RT','Rt')

sample <- industry_df[sample(nrow(industry_df), nrow(industry_df)*0.4), ]
# Splitting the data into train and test sets using categorical splitting
train_split <- sample(1:nrow(sample), 0.8 * nrow(sample))



ind_train_data<-sample%>%
  slice(train_split)

ind_test_data<-sample %>%
  slice(-train_split)

ind_train_data$subject <- sapply(ind_train_data$tweet, extract_subject)
ind_test_data$subject <- sapply(ind_test_data$tweet, extract_subject)



```


```{r}

train_dfm_tweet <-TMEF_dfm(ind_train_data$tweet,ngrams=1:2, min.prop = 0.01)
train_dfm_subject <-TMEF_dfm(ind_train_data$subject,ngrams=1, min.prop = 0.00)
train_dfm <- cbind(train_dfm_tweet, train_dfm_subject)


test_dfm_tweet <-TMEF_dfm(ind_test_data$tweet,
                 ngrams=1:2,
                 min.prop=0) %>%
                 dfm_match(colnames(train_dfm_tweet))

test_dfm_subject <-TMEF_dfm(ind_test_data$subject,
            ngrams=1, min.prop=0) %>%
            dfm_match(colnames(train_dfm_subject))

test_dfm <- cbind(test_dfm_tweet, test_dfm_subject)


train_Y <- ind_train_data$industry
industry_model_with_subject <- cv.glmnet(x=train_dfm,
                                y=train_Y,
                                family="multinomial", alpha = 1)

industry_model <- cv.glmnet(x=train_dfm_tweet,
                                y=train_Y,
                                family="multinomial", alpha = 1)
```


```{r}

industry_predict_label_with_subject<-predict(industry_model_with_subject,
                                newx = test_dfm,
                                type="class")[,1]

industry_predict_label<-predict(industry_model,
                                newx = test_dfm_tweet,
                                type="class")[,1]

mean(industry_predict_label_with_subject == ind_test_data$industry)
mean(industry_predict_label == ind_test_data$industry)
```


```{r}
industry_model_subject_only <- cv.glmnet(x=train_dfm_subject,
                                y=train_Y,
                                family="multinomial", alpha = 1)

industry_predict_label_subject_only<-predict(industry_model_subject_only,
                                newx = test_dfm_subject,
                                type="class")[,1]
```

```{r}
industry_dfm_train_no_comp_names<-TMEF_dfm(ind_train_data$tweet,ngrams=1:2, custom_stop_words = companies, min.prop = 0.01)

industry_dfm_test_no_comp_names <-TMEF_dfm(ind_test_data$tweet,
                                  ngrams=1:2, custom_stop_words = companies, min.prop=0) %>%
                                  dfm_match(colnames(industry_dfm_train_no_comp_names))


industry_model_no_comp_names <- cv.glmnet(x=industry_dfm_train_no_comp_names,
                                y=train_Y,
                                family="multinomial", alpha = 1)


industry_predict_label_no_comp_names<-predict(industry_model_no_comp_names,
                                     newx = industry_dfm_test_no_comp_names,
                                     type="class")[,1]


```

```{r}
bench_mark <- mean("Technology" == ind_test_data$industry)
print(paste0("benckmark_technology:", bench_mark))
acc_model_no_comp_name <- mean(industry_predict_label_no_comp_names == ind_test_data$industry)
print(paste0("acc_model_no_comp_name:", acc_model_no_comp_name))
acc_model_with_subject <- mean(industry_predict_label_with_subject == ind_test_data$industry)
print(paste0("acc_model_with_subject: " ,acc_model_with_subject))
acc_normal_model <- mean(industry_predict_label == ind_test_data$industry)
print(paste0("acc_normal_model: ",acc_normal_model))
acc_model_only_subject <- mean(industry_predict_label_subject_only == ind_test_data$industry)
print(paste0("acc_model_only_subject:", acc_model_only_subject))
```
```{r}
data <- data.frame(
  Method = c("Benchmark- All Technology","Subject Alone", "N-gram Model", "N-gram Model with Subject"),
  Accuracy = c(bench_mark, acc_model_only_subject, acc_normal_model, acc_model_with_subject )
)

data$Method <- factor(data$Method, levels = data$Method[order(-data$Accuracy)])

# Plot
# Plot
ggplot(data, aes(x = Method, y = Accuracy, fill = Method)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = round(Accuracy, 2)), # Add labels for each bar value
            hjust = -0.1, # Adjust horizontal justification
            vjust = 0.5, # Adjust vertical justification
            size = 3) + # Adjust text size
  labs(title = "Accuracy Comparison") + # Swap x and y labels
  coord_flip() + # Flip coordinates to make it horizontal
  theme_minimal() + 
  guides(fill = FALSE) + # Remove legend
  theme(axis.title.x = element_blank(), # Remove x-axis label
        axis.title.y = element_blank()) # Remove y-axis label
```

```{r}
table(industry_predict_label,ind_test_data$industry)
```




```{r}

ind_test_data$predict_1 <- industry_predict_label
ind_test_data$predict_2 <- industry_predict_label_with_subject
indices_auto <- which(industry_predict_label != ind_test_data$industry & industry_predict_label_with_subject == ind_test_data$industry)



# Extract samples where the predicted label is "automotive" but actual label differs , 	

mislabeled_samples_not_auto <- ind_test_data[indices_auto, ]

# View the mislabeled samples
examples_2 <-mislabeled_samples_not_auto %>% filter (V1 %in% c(405288,681439, 564024,397220,819101, 399185, 186295, 51097, 650525)) %>% select(V1, tweet, subject, predict_1,predict_2)

```


```{r}
data <- data.frame(
  Method = c("Benchmark- All Technology", "N-gram Model (with company names as stop words)", "N-gram Model"),
  Accuracy = c(bench_mark, acc_model_no_comp_name, acc_normal_model)
)

data$Method <- factor(data$Method, levels = data$Method[order(-data$Accuracy)])
# Plot
# Plot
ggplot(data, aes(x = Method, y = Accuracy, fill = Method)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = round(Accuracy, 2)), # Add labels for each bar value
            hjust = -0.1, # Adjust horizontal justification
            vjust = 0.5, # Adjust vertical justification
            size = 3) + # Adjust text size
  labs(title = "Accuracy Comparison") + # Swap x and y labels
  coord_flip() + # Flip coordinates to make it horizontal
  theme_minimal() + 
  guides(fill = FALSE) + # Remove legend
  theme(axis.title.x = element_blank(), # Remove x-axis label
        axis.title.y = element_blank(), # Remove y-axis label
        plot.margin = margin(5, 5, 5, 5, "cm"))
```


#Try transfer learning: train on 50 companies, and see how well you predict on another 50... 

```{r}
my_sample <- industry_df[sample(nrow(industry_df), nrow(industry_df)*0.4), ]
print(length(my_sample))
shuffled_stocks <- sample(unique(my_sample$stock))

# Determine the midpoint for splitting
midpoint <- length(shuffled_stocks) / 2

# Split the shuffled stock names into train and test sets
train_stocks <- shuffled_stocks[1:midpoint]
test_stocks <- shuffled_stocks[(midpoint + 1):length(shuffled_stocks)]

# Filter the dataframe based on train and test stocks
train_data <- my_sample[my_sample$stock %in% train_stocks, ]
test_data <- my_sample[my_sample$stock %in% test_stocks, ]

# Print the number of stocks in train and test sets
print(paste("Number of stocks in train set:", nrow(train_data)))
print(paste("Number of stocks in test set:", nrow(test_data)))


```

```{r}
custom_stop <- c('amazon','Facebook', '@amazon', 'netflix', '@netflix', 'apple', '@apple', 'facebook', '@facebook', 'google', '@google', 'rt','RT','Rt')

industry_dfm_train<-TMEF_dfm(train_data$tweet,ngrams=1, custom_stop_words = custom_stop, min.prop = 0.01)

industry_dfm_test<-TMEF_dfm(test_data$tweet,
                           ngrams=1,min.prop=0) %>%
  dfm_match(colnames(industry_dfm_train))
```


```{r}
train_Y <- train_data$industry
industry_model<-
  cv.glmnet(x=industry_dfm_train,
            y=train_Y,
            family="multinomial", alpha = 1)
```

```{r}
# With type="class", you can get a single predicted label for each document
industry_predict_label<-predict(industry_model,
                            newx = industry_dfm_test,
                            type="class")[,1]

# raw accuracy
mean(industry_predict_label==test_data$industry)
mean("Technology"==test_data$industry)

# Confusion matrix - great for multinomials!

table(industry_predict_label,test_data$industry)
```



```{r}
dfm_faang_test_no_company_name<-TMEF_dfm(faang_test_data$tweet,
                               ngrams=1:2,
                               min.prop = 0) %>%
  dfm_match(colnames(dfm_faang_train_no_company_name)) %>%
  convert(to="matrix")



# Evaluate model performance, e.g., accuracy, confusion matrix, etc.
```




```{r}

# Predict on test set
faang_test_predict_pros <- predict(faang_model_pros, 
                                   newx = dfm_faang_test_no_company_name, 
                                   type = "class")[,1]       # Predict class labels
accuracy_score <- mean(faang_test_predict_pros == test_Y)

# Print the accuracy score
print(paste("Accuracy Score:", accuracy_score))
```
```{r}
table(faang_test_predict_pros,Y_test)
```



```{r}
test_predict_binary <-  ifelse( faang_test_predict_pros< 0.46, 0, 1)

round(100*mean(test_predict_binary==test_Y),3)

conf_matrix <- table(Actual = test_Y, Predicted = test_predict_binary)

conf_matrix


TP <- sum(test_predict_binary == 1 & test_Y == 1)
FP <- sum(test_predict_binary == 1 & test_Y == 0)
FN <- sum(test_predict_binary == 0 & test_Y == 1)

# Calculate Precision and Recall
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)

precision
recall
```




```{r}
faang$made_money <- ifelse(faang$"one_day_return" < 0, 0, 1)
table_data <- table(faang$made_money)
percentages <- round((table_data / sum(table_data)) * 100, 2)

# Create bar plot with percentages
barplot(percentages,
        main = "Distribution of 'made_money'",
        xlab = "Value",
        ylab = "Percentage",
        col = "skyblue",
        ylim = c(0, 100),
        names.arg = c("Zero", "One"),
        beside = TRUE)
```



```{r}

calculate_accuracy <- function(threshold, predicted_probs, actual_labels) {
  predicted_labels <- ifelse(predicted_probs >= threshold, 1, 0)
  accuracy <- mean(predicted_labels == actual_labels)
  return(accuracy)
}

# Define a range of thresholds
thresholds <- seq(0, 1, by = 0.001)

# Calculate accuracy for each threshold
accuracies <- sapply(thresholds, function(threshold) {
  calculate_accuracy(threshold, faang_test_predict_pros, test_Y)
})

optimal_threshold <- thresholds[which.max(accuracies)]
max_accuracy <- max(accuracies)

# Plot accuracy against threshold
plot(thresholds, accuracies, type = "l", 
     xlab = "Threshold", ylab = "Accuracy",
     main = "Accuracy vs. Threshold")
abline(v = optimal_threshold, col = "red", lty = 2)


```



```{r}
# lots of zeros
faang_model_pros %>%
  coef() %>%
  drop()

# let's get this in a data frame
faang_model_pros %>%
  coef() %>%
  drop() %>%
  as.data.frame() %>%
  rownames_to_column(var = "ngram") %>%
  rename(score=".")

# just the top
faang_model_pros %>%
  coef() %>%
  drop() %>%
  as.data.frame() %>%
  rownames_to_column(var = "ngram") %>%
  rename(score=".") %>%
  head(20)

# drop zeros, and save
plotCoefs<-faang_model_pros %>%
  coef() %>%
  drop() %>%
  as.data.frame() %>%
  rownames_to_column(var = "ngram") %>%
  rename(score=".") %>%
  filter(score!=0 & ngram!="(Intercept)" & !is.na(score))  

plotCoefs

# create a similar data frame with ngram frequencies
plotFreqs<-data.frame(ngram=colnames(dfm_faang_train),
                      freq=colMeans(dfm_faang_train))


# combine data, round for easy reading
plotDat<-plotCoefs %>%
  left_join(plotFreqs) %>%
  mutate_at(vars(score,freq),~round(.,3))

head(plotDat)



plotDat %>%
  ggplot(aes(x=score,y=freq,label=ngram,color=score)) +
  scale_color_gradient2(low="navyblue",
                        mid = "grey",
                        high="forestgreen",
                        midpoint = 0)+
  geom_vline(xintercept=0)+
  geom_point() +
  geom_label_repel(max.overlaps = 15)+  
  scale_x_continuous(limits = c(-.2,.1),
                     breaks = seq(-.2,.2,.05)) +
  scale_y_continuous(trans="log2",
                     breaks=c(.01,.05,.1,.2,.5,1,2,5))+
  theme_bw() +
  labs(x="Coefficient in Model",y="Uses per Review")+
  theme(legend.position = "none",
        axis.title=element_text(size=20),
        axis.text=element_text(size=16))

ggsave("week2.png",dpi=200,width=20,height=10)
```


#Non-binary
```{r}
faang_train_Y<-faang_train_data %>%
  pull(one_day_return)

test_Y<-faang_test_data %>%
  pull(one_day_return)



faang_model_pros<-cv.glmnet(x=dfm_faang_train,
                             y=faang_train_Y)


faang_test_predict_pros<-predict(faang_model_pros,
                                  newx = dfm_faang_test_pros)[,1]

hist(test_Y)

pros_acc<-kendall_acc(faang_test_predict_pros,test_Y)



```

```{r}
kendalls_acc<-kendall_acc(faang_test_predict_pros,test_Y)

kendalls_acc
```


#Daily Aggregations
```{r}
daily_aggregated <- df %>%
  group_by(day_date = lubridate::floor_date(date, unit = "day"), stock) %>%
  summarize(
    aggregated_tweets = paste(tweet, collapse = " "),
    one_day_return = first(one_day_return),
    num_tweets = n(),
    .groups = "drop_last"
  ) %>%
  ungroup()


daily_aggregated$made_money <- ifelse(daily_aggregated$one_day_return < 0, 0, 1)

class_counts <- table(daily_aggregated$made_money)

# Find the size of the minority class
minority_size <- min(class_counts)

# Undersample the majority class to match the size of the minority class
undersampled_daily <- daily_aggregated %>%
  group_by(made_money) %>%
  sample_n(minority_size) %>%
  ungroup()

# Check the class distribution after undersampling
table(undersampled_daily$made_money)

train_split <- sample(1:nrow(undersampled_daily),0.8*nrow(undersampled_daily))

daily_aggregated_train <-undersampled_daily %>%
  slice(train_split)

daily_aggregated_test <-undersampled_daily %>%
  slice(-train_split)

train_Y<-daily_aggregated_train %>%
  pull(made_money)

test_Y<-daily_aggregated_test %>%
  pull(made_money)

dfm_daily_aggregated<-TMEF_dfm(daily_aggregated_train$aggregated_tweets,ngrams=1:2) %>%
  convert(to="matrix")


```
```{r}
daily_aggregated_model<-cv.glmnet(x=dfm_daily_aggregated,
                             y=train_Y)



```
```{r}
dfm_daily_aggregated_test<-TMEF_dfm(daily_aggregated_test$aggregated_tweets,
                               ngrams=1:2,
                               min.prop = 0) %>%
  dfm_match(colnames(dfm_daily_aggregated)) %>%
  convert(to="matrix")

test_predictions <-predict(daily_aggregated_model,
                                  newx = dfm_daily_aggregated_test)[,1]
```

```{r}

test_predict_binary <-  ifelse(test_predictions < 0.5, 0, 1)
round(100*mean(test_predict_binary==test_Y),3)

conf_matrix <- table(Actual = test_Y, Predicted = test_predict_binary)

conf_matrix


TP <- sum(test_predict_binary == 1 & test_Y == 1)
FP <- sum(test_predict_binary == 1 & test_Y == 0)
FN <- sum(test_predict_binary == 0 & test_Y == 1)

# Calculate Precision and Recall
precision <- TP / (TP + FP)
recall <- TP / (TP + FN)

precision
recall
```




# N-grams weekly
```{r}
weekly_aggregated_faang <- faang %>%
  group_by(week_start_date = lubridate::floor_date(date, unit = "week"), stock) %>%
  summarize(
    aggregated_tweets = paste(tweet, collapse = " "),
    seven_day_return = first(seven_day_return),
    num_tweets = n(),
    .groups = "drop_last"
  ) %>%
  ungroup()
```











```{r}
faang_test_predict_weekly<-predict(faang_model_weekly,
                                  newx = dfm_faang_test_weekly)[,1]


test_predict_binary <-  ifelse( faang_test_predict_weekly < 0.5, 0, 1)

round(100*mean(test_predict_binary==test_Y_weekly),3)

conf_matrix <- table(Actual = test_Y_weekly, Predicted = test_predict_binary)
```
```{r}
test_predict_binary
```
#by sentiment 

```{r}
result <- df %>%
  group_by(stock, date) %>%
  summarize(one_day_return = first(one_day_return), avg_sentiment = mean(LSTM_POLARITY) , .groups = "drop") %>%
  ungroup()
```

```{r}
x <- as.data.frame(result$avg_sentiment)

# Store the response variable in y
y <- result$one_day_return

# Perform cross-validated elastic net regression
sentiment_model <- cv.glmnet(x = x, y = y)
```

```{r}
weekly_aggregated_whole <- df %>%
  group_by(week_start_date = lubridate::floor_date(date, unit = "week"), stock) %>%
  summarize(
    aggregated_tweets = paste(tweet, collapse = " "),
    seven_day_return = first(seven_day_return),
    num_tweets = n(),
    .groups = "drop_last"
  ) %>%
  ungroup()


weekly_aggregated_whole$made_money <- ifelse(weekly_aggregated_whole$seven_day_return < 0, 0, 1)

train_split <- sample(1:nrow(weekly_aggregated_whole),0.8*nrow(weekly_aggregated_whole))

train_data_weekly<-weekly_aggregated_whole %>%
  slice(train_split)

test_data_weekly<-weekly_aggregated_whole %>%
  slice(-train_split)


dfm_train_weekly<-TMEF_dfm(train_data_weekly$aggregated_tweets,ngrams=1:2) %>%
                  convert(to="matrix")

```


```{r}
train_Y_whole<-train_data_weekly %>%
  pull(made_money)

test_Y_weekly<-test_data_weekly %>%
  pull(made_money)

whole_model_weekly <-cv.glmnet(x=dfm_train_weekly,
                             y=train_Y_whole)

dfm_test_weekly<-TMEF_dfm(test_data_weekly$aggregated_tweets,
                               ngrams=1:2,
                               min.prop = 0) %>%
  dfm_match(colnames(dfm_train_weekly)) %>%
  convert(to="matrix")




```


```{r}
whole_test_predict_weekly<-predict(whole_model_weekly,
                                  newx = dfm_test_weekly)[,1]

test_predict_binary <-  ifelse(whole_test_predict_weekly < 0.5, 0, 1)

round(100*mean(test_predict_binary==test_Y_weekly),3)

conf_matrix <- table(Actual = test_Y_weekly, Predicted = test_predict_binary)
```



# Naive Bayes 
```{r}
# Change format of the dataset so it works with the naive bayes function 
naive_bayes_train <- as.data.frame(cbind(dfm_faang_train_pros, faang_train_Y))
naive_bayes_train$faang_train_Y <- as.factor(df_from_array$faang_train_Y)
faang_model_pros <- naive_bayes(faang_train_Y ~ ., data = naive_bayes_train)

# Change format of the test dataset so it works with the naive bayes function 
naive_bayes_test <- as.data.frame(cbind(dfm_faang_test_pros, test_Y))
naive_bayes_test$faang_train_Y <- as.factor(df_from_array$faang_train_Y)

faang_test_predict_pros<-predict(faang_model_pros,newdata = df_test)
head(faang_test_predict_pros)

# Accuracy 
round(100*mean(faang_test_predict_pros==test_Y),3)

```


