```{r}
library(quanteda)
library(ggrepel)
library(textclean)
library(tidyverse)
library(glmnet)
library(pROC)
library(knitr)
library(dplyr)
library(dtplyr)
library(data.table)
library(lubridate)
library(xts)
library(PerformanceAnalytics)
library(knitr)
library(kableExtra)
library(dplyr)
library(sentimentr)
library(doc2concrete)

# undersampling 
library(caret)
library(ROSE)

# r source files 
source("TMEF_dfm.R")
source("vectorFunctions.R") # a new one!

# Classifiers
library(naivebayes)
library(randomForest)
```

# Pre-processing
```{r}
set.seed(2022)
df <- fread("full_dataset-release.csv")
df <- df %>%
  rename(
    tweet = TWEET,
    stock = STOCK,
    one_day_return ="1_DAY_RETURN", 
    seven_day_return = "7_DAY_RETURN",
    date = DATE
  ) %>% 
  mutate(date = as.Date(date, format = "%d/%m/%Y"))
```

```{r}
# remove NAs
rows_with_null <- sum(!complete.cases(df))
df <- na.omit(df)
#Some other data pre-processing: 
# df$tweet <- replace_url(df$tweet)
# df_emoji_text <- df
# # df_emoji_identifier <- df
# df_emoji_text$tweet <- replace_emoji(df$tweet)
# df_emoji_identifier$tweet<- replace_emoji(df$tweet) 
```

# FANG Preprocessing 
```{r}
custom_stop <- c('amazon','Facebook', '@amazon', 'netflix', '@netflix', 'apple', '@apple', 'facebook', '@facebook', 'google', '@google', 'rt','RT','Rt')

faang <- df %>%
  filter(stock %in% c("Facebook", "Apple", "Amazon", "Netflix", "Google"))
faang$tweet <- replace_emoji(faang$tweet)
faang$tweet <- replace_url(faang$tweet)

faang$made_money <- ifelse(faang$"one_day_return" < 0, 0, 1)

```


# FAANG models (Up / Down)
```{r}

# Split train test dataset 
train_split <- sample(1:nrow(faang),0.8 * nrow(faang))

# Training data 
train_X <-faang %>%
  slice(train_split)

train_Y <- train_X %>%
  pull(made_money)

# Test Data 
test_X <-faang %>%
  slice(-train_split)

test_Y <-test_X %>%
  pull(made_money)

# Create a DFM of the train dataset 
dfm_train <-TMEF_dfm(train_X$tweet,ngrams=1:2,custom_stop_words = custom_stop) %>%
  convert(to="matrix")

# Read DFM from RDS file instead 
# dfm_faang_train<-readRDS("dfm_faang_train.RDS")
# dfm_faang_test<-readRDS("dfm_faang_test.RDS")

faang_model<-cv.glmnet(x=dfm_train,
                             y=train_Y)

dfm_test <-TMEF_dfm(test_X$tweet,
                               ngrams=1:2,
                               min.prop = 0,
                    custom_stop_words = custom_stop) %>%
  dfm_match(colnames(dfm_train)) %>%
  convert(to="matrix")

# # Save DFM into RDS file 
# saveRDS(dfm_train,file="dfm_faang_train.RDS")
# saveRDS(dfm_test,file="dfm_faang_test.RDS")/ 

test_predict <- predict(faang_model,
                                  newx = dfm_test)[,1]

test_predict_binary=ifelse(test_predict > 0.5,
                           1,
                           0)

kendall_acc(test_predict_binary,test_Y)

# Accuracy 
round(100*mean(test_predict_binary==test_Y),3)

# extract coefficients
plotCoefs<-faang_model %>%
  coef() %>%
  drop() %>%
  as.data.frame() %>%
  rownames_to_column(var = "ngram") %>%
  rename(score=".") %>%
  filter(score!=0 & ngram!="(Intercept)" & !is.na(score))  

# merge frequencies
plotDat<-plotCoefs %>%
  left_join(data.frame(ngram=colnames(dfm_test),
                       freq=colMeans(dfm_test))) %>%
  mutate_at(vars(score,freq),~round(.,3))

# pipe into ggplot
plotDat %>%
  ggplot(aes(x=score,y=freq,label=ngram,color=score)) +
  scale_color_gradient2(low="navyblue",
                        mid = "grey",
                        high="forestgreen",
                        midpoint = 0)+
  geom_vline(xintercept=0)+
  geom_point() +
  geom_label_repel(max.overlaps = 15)+  
  scale_x_continuous(limits = c(-.2,.1),
                     breaks = seq(-.2,.2,.05)) +
  scale_y_continuous(trans="log2",
                     breaks=c(.01,.05,.1,.2,.5,1,2,5))+
  theme_bw() +
  labs(x="Coefficient in Star Rating Model",y="Uses per Review")+
  theme(legend.position = "none",
        axis.title=element_text(size=20),
        axis.text=element_text(size=16))

```

# Facebook Model and accuracy (Up Down) 
```{r}

facebook <- faang %>%
  filter(stock %in% c("Amazon"))

print(paste("Number of rows:", nrow(facebook)))

# Split train test dataset 
train_split <- sample(1:nrow(facebook),0.8 * nrow(facebook))

train_X<-facebook%>%
  slice(train_split)

test_X<-facebook %>%
  slice(-train_split)

train_Y<-train_X %>%
  pull(made_money)

test_Y<-test_X %>%
  pull(made_money)

# Create a DFM of the train dataset 
facebook_train <-TMEF_dfm(train_X$tweet,ngrams=1:2) %>%
  convert(to="matrix")

facebook_model <-cv.glmnet(x=facebook_train,
                             y=train_Y)

facebook_test <-TMEF_dfm(test_X$tweet,
                               ngrams=1:2,
                               min.prop = 0) %>%
  dfm_match(colnames(facebook_train)) %>%
  convert(to="matrix")

facebook_test_predict <-predict(facebook_model,
                                  newx = facebook_test)[,1]

# dfm_faang_train<-readRDS("facebook_train.RDS")
# dfm_faang_test<-readRDS("facebook_train.RDS")

# saveRDS(facebook_train,file="facebook_train.RDS")
# saveRDS(facebook_test,file="facebook_test.RDS")

kendall_acc(facebook_test_predict,test_Y)


test_predict_binary=ifelse(facebook_test_predict>0.5,
                           1,
                           0) 
# Accuracy 
round(100*mean(test_predict_binary==test_Y),3)

# extract coefficients
plotCoefs<-faang_model %>%
  coef() %>%
  drop() %>%
  as.data.frame() %>%
  rownames_to_column(var = "ngram") %>%
  rename(score=".") %>%
  filter(score!=0 & ngram!="(Intercept)" & !is.na(score))  

# merge frequencies
plotDat<-plotCoefs %>%
  left_join(data.frame(ngram=colnames(facebook_test),
                       freq=colMeans(facebook_test))) %>%
  mutate_at(vars(score,freq),~round(.,3))

# pipe into ggplot
plotDat %>%
  ggplot(aes(x=score,y=freq,label=ngram,color=score)) +
  scale_color_gradient2(low="navyblue",
                        mid = "grey",
                        high="forestgreen",
                        midpoint = 0)+
  geom_vline(xintercept=0)+
  geom_point() +
  geom_label_repel(max.overlaps = 15)+  
  scale_x_continuous(limits = c(-.2,.1),
                     breaks = seq(-.2,.2,.05)) +
  scale_y_continuous(trans="log2",
                     breaks=c(.01,.05,.1,.2,.5,1,2,5))+
  theme_bw() +
  labs(x="Coefficient in Star Rating Model",y="Uses per Review")+
  theme(legend.position = "none",
        axis.title=element_text(size=20),
        axis.text=element_text(size=16))

```

# Applying Word Vector to Facebook data
```{r}

# Read the word to vec file 
# word vector 
vecSmall<-readRDS("vecSmall.RDS")
load("wfFile.RData")

train_split <- sample(1:nrow(faang),0.8 * nrow(faang))

train_data<-faang%>%
  slice(train_split)

test_data<-faang %>%
  slice(-train_split)

vdat <- readRDS("facebook_vdat.RDS")


# # project data onto the word vector
# vdat<-vecCheck(faang$tweet,
               #vecSmall,
               #wfFile,
               #PCAtrim=1)

#vdat <- readRDS("faang_vdat.RDS")

# vdat <- readRDS("faang_vdat.RDS")
# saveRDS(vdat,file="facebook_vdat.RDS")

vdat_train<-vdat[train_split,]
vdat_test<-vdat[-train_split,]
# Train a vector classifier
lasso_vec<-glmnet::cv.glmnet(x=vdat_train,
                             y=train_data$made_money)

# notice two lines - one is at the minimum, the other is more conservative 
plot(lasso_vec)

# the default chooses the more conservative one, with fewer features
test_all_predict<-predict(lasso_vec,
                          newx = vdat_test)

kendall_acc(test_all_predict,test_data$made_money)

# this is how you use the minimum one - usually it produces better accuracy
test_vec_predict<-predict(lasso_vec,newx = vdat_test,
                          s="lambda.min")

kendall_acc(test_vec_predict,test_data$made_money)

```

```{r}
train_split <- sample(1:nrow(faang),0.8 * nrow(faang))

train_data<-faang%>%
  slice(train_split)

test_data<-faang %>%
  slice(-train_split)

positive_dict<-textdata::lexicon_loughran() %>%
  filter(sentiment=="positive") %>%
  pull(word) %>%
  paste(collapse=" ")

pos_dict<-list(uc=textdata::lexicon_loughran() %>%
                 filter(sentiment=="positive") %>%
                 pull(word)) %>%
  dictionary()

test_data$speech_wdct <- test_data$tweet %>%
  tokens() %>%
  tokens_ngrams(n = 1) %>%
  sapply(length)

pos_dict_bow<-test_data$tweet %>%
  tokens() %>%
  dfm() %>%
  dfm_lookup(pos_dict) %>%
  convert(to="matrix") %>%
  apply(2, function(x) x/test_data$speech_wdct)

pos_sims <- readRDS("pos_sims.rds")

#pos_sims<-vecSimCalc(x=test_data$tweet,
                  #y=positive_dict,
                  #vecfile=vecSmall,
                  #wffile = wfFile,
                  #PCAtrim = 1)

#saveRDS(pos_sims, "pos_sims.rds")
```

```{r}

acc_possims<-kendall_acc(test_data$made_money,pos_sims)

acc_possims

acc_posbow<-kendall_acc(test_data$made_money,pos_dict_bow)

acc_posbow
```

```{r}
uncertain_dict<-textdata::lexicon_loughran() %>%
  filter(sentiment=="uncertainty") %>%
  pull(word) %>%
  paste(collapse=" ")

unc_dict<-list(uc=textdata::lexicon_loughran() %>%
                 filter(sentiment=="uncertainty") %>%
                 pull(word)) %>%
  dictionary()

test_data$speech_wdct <- test_data$tweet %>%
  tokens() %>%
  tokens_ngrams(n = 1) %>%
  sapply(length)

uncertain_dict_bow<-test_data$tweet %>%
  tokens() %>%
  dfm() %>%
  dfm_lookup(unc_dict) %>%
  convert(to="matrix") %>%
  apply(2, function(x) x/test_data$speech_wdct)

#pos_sims <- readRDS("pos_sims.rds")

uncertain_sims<-vecSimCalc(x=test_data$tweet,
                  y=uncertain_dict,
                  vecfile=vecSmall,
                  wffile = wfFile,
                  PCAtrim = 1)

saveRDS(uncertain_sims, "uncertain_sims.rds")

acc_uncsims<-kendall_acc(test_data$made_money,uncertain_sims)

acc_uncsims

acc_uncbow<-kendall_acc(test_data$made_money,uncertain_dict_bow)

acc_uncbow
```
```